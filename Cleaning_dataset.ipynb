{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dataset=pd.read_csv('attacks.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first sight we can see a 23 columns dataset. Some of them are dates, some of them are categorical data and links to files than can be useful if important information is missed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case Number</th>\n",
       "      <th>Date</th>\n",
       "      <th>Year</th>\n",
       "      <th>Type</th>\n",
       "      <th>Country</th>\n",
       "      <th>Area</th>\n",
       "      <th>Location</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Injury</th>\n",
       "      <th>Fatal (Y/N)</th>\n",
       "      <th>Time</th>\n",
       "      <th>Species</th>\n",
       "      <th>Investigator or Source</th>\n",
       "      <th>pdf</th>\n",
       "      <th>href formula</th>\n",
       "      <th>href</th>\n",
       "      <th>Case Number.1</th>\n",
       "      <th>Case Number.2</th>\n",
       "      <th>original order</th>\n",
       "      <th>Unnamed: 22</th>\n",
       "      <th>Unnamed: 23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018.06.25</td>\n",
       "      <td>25-Jun-2018</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>Boating</td>\n",
       "      <td>USA</td>\n",
       "      <td>California</td>\n",
       "      <td>Oceanside, San Diego County</td>\n",
       "      <td>Paddling</td>\n",
       "      <td>Julie Wolfe</td>\n",
       "      <td>F</td>\n",
       "      <td>57</td>\n",
       "      <td>No injury to occupant, outrigger canoe and pad...</td>\n",
       "      <td>N</td>\n",
       "      <td>18h00</td>\n",
       "      <td>White shark</td>\n",
       "      <td>R. Collier, GSAF</td>\n",
       "      <td>2018.06.25-Wolfe.pdf</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>2018.06.25</td>\n",
       "      <td>2018.06.25</td>\n",
       "      <td>6303.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018.06.18</td>\n",
       "      <td>18-Jun-2018</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>St. Simon Island, Glynn County</td>\n",
       "      <td>Standing</td>\n",
       "      <td>AdysonÂ McNeely</td>\n",
       "      <td>F</td>\n",
       "      <td>11</td>\n",
       "      <td>Minor injury to left thigh</td>\n",
       "      <td>N</td>\n",
       "      <td>14h00  -15h00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>K.McMurray, TrackingSharks.com</td>\n",
       "      <td>2018.06.18-McNeely.pdf</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>2018.06.18</td>\n",
       "      <td>2018.06.18</td>\n",
       "      <td>6302.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018.06.09</td>\n",
       "      <td>09-Jun-2018</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>Invalid</td>\n",
       "      <td>USA</td>\n",
       "      <td>Hawaii</td>\n",
       "      <td>Habush, Oahu</td>\n",
       "      <td>Surfing</td>\n",
       "      <td>John Denges</td>\n",
       "      <td>M</td>\n",
       "      <td>48</td>\n",
       "      <td>Injury to left lower leg from surfboard skeg</td>\n",
       "      <td>N</td>\n",
       "      <td>07h45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>K.McMurray, TrackingSharks.com</td>\n",
       "      <td>2018.06.09-Denges.pdf</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>2018.06.09</td>\n",
       "      <td>2018.06.09</td>\n",
       "      <td>6301.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018.06.08</td>\n",
       "      <td>08-Jun-2018</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>New South Wales</td>\n",
       "      <td>Arrawarra Headland</td>\n",
       "      <td>Surfing</td>\n",
       "      <td>male</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Minor injury to lower leg</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2 m shark</td>\n",
       "      <td>B. Myatt, GSAF</td>\n",
       "      <td>2018.06.08-Arrawarra.pdf</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>2018.06.08</td>\n",
       "      <td>2018.06.08</td>\n",
       "      <td>6300.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018.06.04</td>\n",
       "      <td>04-Jun-2018</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>Provoked</td>\n",
       "      <td>MEXICO</td>\n",
       "      <td>Colima</td>\n",
       "      <td>La Ticla</td>\n",
       "      <td>Free diving</td>\n",
       "      <td>Gustavo Ramos</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lacerations to leg &amp; hand shark PROVOKED INCIDENT</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tiger shark, 3m</td>\n",
       "      <td>A .Kipper</td>\n",
       "      <td>2018.06.04-Ramos.pdf</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>http://sharkattackfile.net/spreadsheets/pdf_di...</td>\n",
       "      <td>2018.06.04</td>\n",
       "      <td>2018.06.04</td>\n",
       "      <td>6299.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25718</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25719</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25720</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25721</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25722</th>\n",
       "      <td>xx</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25723 rows Ã 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Case Number         Date    Year        Type    Country  \\\n",
       "0      2018.06.25  25-Jun-2018  2018.0     Boating        USA   \n",
       "1      2018.06.18  18-Jun-2018  2018.0  Unprovoked        USA   \n",
       "2      2018.06.09  09-Jun-2018  2018.0     Invalid        USA   \n",
       "3      2018.06.08  08-Jun-2018  2018.0  Unprovoked  AUSTRALIA   \n",
       "4      2018.06.04  04-Jun-2018  2018.0    Provoked     MEXICO   \n",
       "...           ...          ...     ...         ...        ...   \n",
       "25718         NaN          NaN     NaN         NaN        NaN   \n",
       "25719         NaN          NaN     NaN         NaN        NaN   \n",
       "25720         NaN          NaN     NaN         NaN        NaN   \n",
       "25721         NaN          NaN     NaN         NaN        NaN   \n",
       "25722          xx          NaN     NaN         NaN        NaN   \n",
       "\n",
       "                  Area                        Location     Activity  \\\n",
       "0           California     Oceanside, San Diego County     Paddling   \n",
       "1              Georgia  St. Simon Island, Glynn County     Standing   \n",
       "2               Hawaii                    Habush, Oahu      Surfing   \n",
       "3      New South Wales              Arrawarra Headland      Surfing   \n",
       "4               Colima                        La Ticla  Free diving   \n",
       "...                ...                             ...          ...   \n",
       "25718              NaN                             NaN          NaN   \n",
       "25719              NaN                             NaN          NaN   \n",
       "25720              NaN                             NaN          NaN   \n",
       "25721              NaN                             NaN          NaN   \n",
       "25722              NaN                             NaN          NaN   \n",
       "\n",
       "                  Name Sex   Age  \\\n",
       "0          Julie Wolfe    F   57   \n",
       "1      AdysonÂ McNeely     F   11   \n",
       "2          John Denges    M   48   \n",
       "3                 male    M  NaN   \n",
       "4       Gustavo Ramos     M  NaN   \n",
       "...                ...  ...  ...   \n",
       "25718              NaN  NaN  NaN   \n",
       "25719              NaN  NaN  NaN   \n",
       "25720              NaN  NaN  NaN   \n",
       "25721              NaN  NaN  NaN   \n",
       "25722              NaN  NaN  NaN   \n",
       "\n",
       "                                                  Injury Fatal (Y/N)  \\\n",
       "0      No injury to occupant, outrigger canoe and pad...           N   \n",
       "1                             Minor injury to left thigh           N   \n",
       "2           Injury to left lower leg from surfboard skeg           N   \n",
       "3                              Minor injury to lower leg           N   \n",
       "4      Lacerations to leg & hand shark PROVOKED INCIDENT           N   \n",
       "...                                                  ...         ...   \n",
       "25718                                                NaN         NaN   \n",
       "25719                                                NaN         NaN   \n",
       "25720                                                NaN         NaN   \n",
       "25721                                                NaN         NaN   \n",
       "25722                                                NaN         NaN   \n",
       "\n",
       "                Time         Species           Investigator or Source  \\\n",
       "0              18h00      White shark                R. Collier, GSAF   \n",
       "1      14h00  -15h00              NaN  K.McMurray, TrackingSharks.com   \n",
       "2              07h45              NaN  K.McMurray, TrackingSharks.com   \n",
       "3                NaN        2 m shark                  B. Myatt, GSAF   \n",
       "4                NaN  Tiger shark, 3m                       A .Kipper   \n",
       "...              ...              ...                             ...   \n",
       "25718            NaN              NaN                             NaN   \n",
       "25719            NaN              NaN                             NaN   \n",
       "25720            NaN              NaN                             NaN   \n",
       "25721            NaN              NaN                             NaN   \n",
       "25722            NaN              NaN                             NaN   \n",
       "\n",
       "                            pdf  \\\n",
       "0          2018.06.25-Wolfe.pdf   \n",
       "1        2018.06.18-McNeely.pdf   \n",
       "2         2018.06.09-Denges.pdf   \n",
       "3      2018.06.08-Arrawarra.pdf   \n",
       "4          2018.06.04-Ramos.pdf   \n",
       "...                         ...   \n",
       "25718                       NaN   \n",
       "25719                       NaN   \n",
       "25720                       NaN   \n",
       "25721                       NaN   \n",
       "25722                       NaN   \n",
       "\n",
       "                                            href formula  \\\n",
       "0      http://sharkattackfile.net/spreadsheets/pdf_di...   \n",
       "1      http://sharkattackfile.net/spreadsheets/pdf_di...   \n",
       "2      http://sharkattackfile.net/spreadsheets/pdf_di...   \n",
       "3      http://sharkattackfile.net/spreadsheets/pdf_di...   \n",
       "4      http://sharkattackfile.net/spreadsheets/pdf_di...   \n",
       "...                                                  ...   \n",
       "25718                                                NaN   \n",
       "25719                                                NaN   \n",
       "25720                                                NaN   \n",
       "25721                                                NaN   \n",
       "25722                                                NaN   \n",
       "\n",
       "                                                    href Case Number.1  \\\n",
       "0      http://sharkattackfile.net/spreadsheets/pdf_di...    2018.06.25   \n",
       "1      http://sharkattackfile.net/spreadsheets/pdf_di...    2018.06.18   \n",
       "2      http://sharkattackfile.net/spreadsheets/pdf_di...    2018.06.09   \n",
       "3      http://sharkattackfile.net/spreadsheets/pdf_di...    2018.06.08   \n",
       "4      http://sharkattackfile.net/spreadsheets/pdf_di...    2018.06.04   \n",
       "...                                                  ...           ...   \n",
       "25718                                                NaN           NaN   \n",
       "25719                                                NaN           NaN   \n",
       "25720                                                NaN           NaN   \n",
       "25721                                                NaN           NaN   \n",
       "25722                                                NaN           NaN   \n",
       "\n",
       "      Case Number.2  original order Unnamed: 22 Unnamed: 23  \n",
       "0        2018.06.25          6303.0         NaN         NaN  \n",
       "1        2018.06.18          6302.0         NaN         NaN  \n",
       "2        2018.06.09          6301.0         NaN         NaN  \n",
       "3        2018.06.08          6300.0         NaN         NaN  \n",
       "4        2018.06.04          6299.0         NaN         NaN  \n",
       "...             ...             ...         ...         ...  \n",
       "25718           NaN             NaN         NaN         NaN  \n",
       "25719           NaN             NaN         NaN         NaN  \n",
       "25720           NaN             NaN         NaN         NaN  \n",
       "25721           NaN             NaN         NaN         NaN  \n",
       "25722           NaN             NaN         NaN         NaN  \n",
       "\n",
       "[25723 rows x 24 columns]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a copy of the original dataset for security"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dataset=original_dataset.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking for NaN values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we are going to check the two last columns that appears to be only NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Since the original data set has 25723 rows and the column Unnamed:22 has 25722  NaN values we can safely drop it.\n"
     ]
    }
   ],
   "source": [
    "print(f'Since the original data set has {cleaned_dataset[\"Unnamed: 22\"].size} rows and the column Unnamed:22 \\\n",
    "has {sum(cleaned_dataset[\"Unnamed: 22\"].isna())}  NaN values we can safely drop it.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Since the original data set has 25723 rows and the column Unnamed:23 has 25721  NaN values we can safely drop it.\n"
     ]
    }
   ],
   "source": [
    "print(f'Since the original data set has {cleaned_dataset[\"Unnamed: 23\"].size} rows and the column Unnamed:23 \\\n",
    "has {sum(cleaned_dataset[\"Unnamed: 23\"].isna())}  NaN values we can safely drop it.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dataset=cleaned_dataset.drop(columns=[\"Unnamed: 23\",\"Unnamed: 22\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell bellow we can see that we are not almost near to detect all the missing values. Comparing the number of NaN to the number of data, it seems that there are lots of rows that are empty. Let's check that by creating a new column that shows how many NaN values has each row,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Case Number               17021\n",
       "Date                      19421\n",
       "Year                      19423\n",
       "Type                      19425\n",
       "Country                   19471\n",
       "Area                      19876\n",
       "Location                  19961\n",
       "Activity                  19965\n",
       "Name                      19631\n",
       "Sex                       19986\n",
       "Age                       22252\n",
       "Injury                    19449\n",
       "Fatal (Y/N)               19960\n",
       "Time                      22775\n",
       "Species                   22259\n",
       "Investigator or Source    19438\n",
       "pdf                       19421\n",
       "href formula              19422\n",
       "href                      19421\n",
       "Case Number.1             19421\n",
       "Case Number.2             19421\n",
       "original order            19414\n",
       "dtype: int64"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22    17020\n",
       "21     2394\n",
       "1      1516\n",
       "0      1422\n",
       "2      1200\n",
       "3      1196\n",
       "4       540\n",
       "5       293\n",
       "6       102\n",
       "7        26\n",
       "8         7\n",
       "20        7\n",
       "Name: Is_all_NaN, dtype: int64"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_dataset[\"Is_all_NaN\"]=cleaned_dataset.isna().sum(axis=1)\n",
    "\n",
    "cleaned_dataset[\"Is_all_NaN\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that there are a great number of columns that have no information or almost no information (columns with 22 or 21). We proceed to remove them. Afther, we will drop the generated \"Is_all_NaN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dataset=cleaned_dataset.dropna(axis=0, how='any', thresh=20, subset=None, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dataset=cleaned_dataset.drop(columns=\"Is_all_NaN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are still some NaN values, but we will with that later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Case Number                  0\n",
       "Date                         0\n",
       "Year                         0\n",
       "Type                         1\n",
       "Country                      2\n",
       "Area                       103\n",
       "Location                   144\n",
       "Activity                   153\n",
       "Name                        24\n",
       "Sex                        217\n",
       "Age                       1929\n",
       "Injury                       2\n",
       "Fatal (Y/N)                362\n",
       "Time                      2422\n",
       "Species                   2139\n",
       "Investigator or Source       5\n",
       "pdf                          0\n",
       "href formula                 1\n",
       "href                         0\n",
       "Case Number.1                0\n",
       "Case Number.2                0\n",
       "original order               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duplicated values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are looking for rows that have duplicated values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(cleaned_dataset.duplicated())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are not rows with duplicated values. Now we look for columns with duplicated values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case Number</th>\n",
       "      <th>Date</th>\n",
       "      <th>Case Number.1</th>\n",
       "      <th>Case Number.2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018.06.25</td>\n",
       "      <td>25-Jun-2018</td>\n",
       "      <td>2018.06.25</td>\n",
       "      <td>2018.06.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018.06.18</td>\n",
       "      <td>18-Jun-2018</td>\n",
       "      <td>2018.06.18</td>\n",
       "      <td>2018.06.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018.06.09</td>\n",
       "      <td>09-Jun-2018</td>\n",
       "      <td>2018.06.09</td>\n",
       "      <td>2018.06.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018.06.08</td>\n",
       "      <td>08-Jun-2018</td>\n",
       "      <td>2018.06.08</td>\n",
       "      <td>2018.06.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018.06.04</td>\n",
       "      <td>04-Jun-2018</td>\n",
       "      <td>2018.06.04</td>\n",
       "      <td>2018.06.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6290</th>\n",
       "      <td>ND.0012</td>\n",
       "      <td>Before 19-Jul-1913</td>\n",
       "      <td>ND.0012</td>\n",
       "      <td>ND.0012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6296</th>\n",
       "      <td>ND.0006</td>\n",
       "      <td>Before 1906</td>\n",
       "      <td>ND.0006</td>\n",
       "      <td>ND.0006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6297</th>\n",
       "      <td>ND.0005</td>\n",
       "      <td>Before 1903</td>\n",
       "      <td>ND.0005</td>\n",
       "      <td>ND.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6299</th>\n",
       "      <td>ND.0003</td>\n",
       "      <td>1900-1905</td>\n",
       "      <td>ND.0003</td>\n",
       "      <td>ND.0003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6301</th>\n",
       "      <td>ND.0001</td>\n",
       "      <td>1845-1853</td>\n",
       "      <td>ND.0001</td>\n",
       "      <td>ND.0001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5334 rows Ã 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Case Number                Date Case Number.1 Case Number.2\n",
       "0     2018.06.25         25-Jun-2018    2018.06.25    2018.06.25\n",
       "1     2018.06.18         18-Jun-2018    2018.06.18    2018.06.18\n",
       "2     2018.06.09         09-Jun-2018    2018.06.09    2018.06.09\n",
       "3     2018.06.08         08-Jun-2018    2018.06.08    2018.06.08\n",
       "4     2018.06.04         04-Jun-2018    2018.06.04    2018.06.04\n",
       "...          ...                 ...           ...           ...\n",
       "6290     ND.0012  Before 19-Jul-1913       ND.0012       ND.0012\n",
       "6296     ND.0006         Before 1906       ND.0006       ND.0006\n",
       "6297     ND.0005         Before 1903       ND.0005       ND.0005\n",
       "6299     ND.0003           1900-1905       ND.0003       ND.0003\n",
       "6301     ND.0001           1845-1853       ND.0001       ND.0001\n",
       "\n",
       "[5334 rows x 4 columns]"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_dataset[['Case Number','Date','Case Number.1','Case Number.2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dataset=cleaned_dataset.drop(columns=['Case Number.1','Case Number.2'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So with \"Case Number\" it was pretty easy to compare by sight, but it is more difficult to compare URLs by sight, so we will do it by checking with boolean logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     5282\n",
       "False      52\n",
       "dtype: int64"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(cleaned_dataset['href formula']==cleaned_dataset['href']).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that both columns are the same but some mistakes had been made. Nonetheless, we cannot drop a column arbitrary, so lets check the values that are not equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  href  \\\n",
      "50   http://sharkattackfile.net/spreadsheets/pdf_di...   \n",
      "96   http://sharkattackfile.net/spreadsheets/pdf_di...   \n",
      "131  http://sharkattackfile.net/spreadsheets/pdf_di...   \n",
      "133  http://sharkattackfile.net/spreadsheets/pdf_di...   \n",
      "141  http://sharkattackfile.net/spreadsheets/pdf_di...   \n",
      "\n",
      "                                          href formula  \n",
      "50   http://sharkattackfile.net/spreadsheets/pdf_di...  \n",
      "96   http://sharkattackfile.net/spreadsheets/pdf_di...  \n",
      "131  http://sharkattackfile.net/spreadsheets/pdf_di...  \n",
      "133  http://sharkattackfile.net/spreadsheets/pdf_di...  \n",
      "141  http://sharkattackfile.net/spreadsheets/pdf_di...  \n"
     ]
    }
   ],
   "source": [
    "pprint(cleaned_dataset[['href','href formula']].loc[cleaned_dataset['href']!=cleaned_dataset['href formula']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When comparing the columns that have different values, we find that none of the URLs are operative anymore. The same is true for the columns that have equal values. This column can be dropped without losing any practical information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dataset=cleaned_dataset.drop(columns=['href','href formula'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are finished (at least by the moment) with deleting rows and columns we are going to reset the dataframe indexes in sake of clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dataset=cleaned_dataset.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consistency of the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we are going to check whether each column has usefull and well formated information or no. This only can be done in a column by column basis, since no general criteria can be applied. Also, we will check if the datatype is the appropiate for this kind of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'Case Number', 'Date', 'Year', 'Type', 'Country', 'Area',\n",
       "       'Location', 'Activity', 'Name', 'Sex ', 'Age', 'Injury', 'Fatal (Y/N)',\n",
       "       'Time', 'Species ', 'Investigator or Source', 'pdf', 'original order'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1957           10\n",
       "1950            6\n",
       "12-Apr-2001     5\n",
       "1958            5\n",
       "05-Oct-2003     5\n",
       "               ..\n",
       "12-Mar-1961     1\n",
       "22-Oct-1988     1\n",
       "Mar-1973        1\n",
       "26-Sep-1994     1\n",
       "14-Aug-2002     1\n",
       "Name: Date, Length: 4652, dtype: int64"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_dataset['Date'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date column doesn't seem to have well formatted data. There is some uncompleted information about dates, but it still can be usefull in the future. Let's check next columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Year column is a more simple column and looks better than date column. However, when we check the minimun of the dataset, we have 0 values. Since we can hardly believe that we have data from year 0, let's try to complete this column with information from \"Date\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2015.0    139\n",
       "2017.0    127\n",
       "2011.0    124\n",
       "2016.0    123\n",
       "2014.0    116\n",
       "         ... \n",
       "1703.0      1\n",
       "1859.0      1\n",
       "1779.0      1\n",
       "1748.0      1\n",
       "1816.0      1\n",
       "Name: Year, Length: 201, dtype: int64"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_dataset['Year'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5253         Ca. 336.B.C..\n",
       "5254          Ca. 725 B.C.\n",
       "5255          1990 or 1991\n",
       "5256           Before 2016\n",
       "5257       Before Oct-2009\n",
       "               ...        \n",
       "5329    Before 19-Jul-1913\n",
       "5330           Before 1906\n",
       "5331           Before 1903\n",
       "5332             1900-1905\n",
       "5333             1845-1853\n",
       "Name: Date, Length: 81, dtype: object"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_dataset['Date'].loc[cleaned_dataset['Year']==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there is some related information in the column Date when Year is 0, but something comes to the eye. All the data that has 0 in year are acumulated at the end of the dataset(You can see this by looking to the indexes). Seems that those data are added at posteriori and has no consistency with the dataset. We will make further comprobations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5253 values with a value different from 0 grouped in 200 categories (which is a very reasonable number for a period of time)\n"
     ]
    }
   ],
   "source": [
    "print(f'There are {cleaned_dataset[\"Year\"].loc[cleaned_dataset[\"Year\"]!=0].size} values with a value different from 0 grouped in {cleaned_dataset[\"Year\"].loc[cleaned_dataset[\"Year\"]!=0].value_counts().size} categories (which is a very reasonable number for a period of time)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 81 values with a value equal to 0 from a total sample size of 5334\n"
     ]
    }
   ],
   "source": [
    "print(f'There are {cleaned_dataset[\"Year\"].loc[cleaned_dataset[\"Year\"]==0].size} values with a value equal to 0 from a total sample size of {cleaned_dataset[\"Year\"].size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's clear that those data are not realiable and we can drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dataset=cleaned_dataset[cleaned_dataset.Year!=0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also Year data need to be casted to integer values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dataset['Year']=cleaned_dataset['Year'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
